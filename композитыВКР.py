# -*- coding: utf-8 -*-
"""Композиты_ipynb_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PPxFOD4M34GNi-MmdH1J8AP0fnvsurKk

Импотрт библиотек
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sns
import tensorflow as tf
import sklearn
import pickle
import scipy
from tensorflow import keras
from keras import layers
from keras.layers import Dense, Flatten, Dropout, BatchNormalization, Activation
from keras.models import Sequential
from keras.layers import Dense,LeakyReLU,ReLU,Softmax
from keras.optimizers import Adam
from sklearn import linear_model
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression, SGDRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error, mean_absolute_error
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn import preprocessing
from sklearn.preprocessing import Normalizer, LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor

"""Загрузка датасетов"""

df_bp = pd.read_excel('/content/drive/MyDrive/Датасет композиты/X_bp.xlsx')
df_bp.shape # размерность

from google.colab import drive
drive.mount('/content/drive')

df_bp.head() # Данные в таблице

#Удаляем первый неинформативный столбец
df_bp.drop(['Unnamed: 0'], axis=1, inplace=True)
df_bp.head()

"""Размерности"""

df_bp.shape

df_nup = pd.read_excel('/content/drive/MyDrive/Датасет композиты/X_nup.xlsx')
df_nup.shape

df_nup.head()

df_nup.drop(['Unnamed: 0'], axis=1, inplace=True) # Удаляем первый столбец из второй таблицы
df_nup.head()

df_nup.shape

"""Сделаем объединение по индексу,тип INNER"""

df = df_bp.merge(df_nup, left_index=True, right_index=True, how='inner')
df.head()

df = df_bp.merge(df_nup, left_index=True, right_index=True, how='inner')
df.head().T

"""Размерность датасета

"""

df.shape

"""Общая информация о датасете,тип данных"""

df.info()

"""Итого 12 типов с плавающей точкой и 1 целочисленное

Количество пропусков
"""

df.isna().sum()

"""Количество уникальных значений"""

df.nunique()

df.duplicated().sum() # Наличие дубликатов

df

"""В параметре "угол нашивки" только 2 значения, поэтому для удобства мы можем определить переменные как 0 и 1"""

df = df.replace({'Угол нашивки, град': {0.0 : 0, 90.0 : 1}})
df['Угол нашивки, град'] = df['Угол нашивки, град'].astype(int)

df = df.rename(columns={'Угол нашивки, град' : 'Угол нашивки'}) # Переименование столбца
df

#Описательная статистика датасета
# count - количество значений
#mean - среднее значение
#std - стандартное отклонение
#min - минимум
#25% - верхнее значение первого квартиля
#50% - медиана
#75% - верхнее значение третьего квартиля
#max - максимум
df.describe().T

df.isnull().sum() # Пропуски данных

"""Среднее и медианное значения для каждого столбца"""

df.mean()

df.median()

df.hist(figsize = (15,15), color = "y")
plt.show()

"""Гистограммы распределения параметров переменных и "ящик с усами"
"""

fig, axes = plt.subplots(13, 2, figsize=(15, 40))
for k, column in enumerate(df.columns):    
    sns.histplot(data=df, x=column, kde=True, ax=axes[k, 0])
    sns.boxplot(data=df, x=column, ax=axes[k, 1])
plt.show()

"""Все признаки, кроме "Угол нашивки" имеют нормальное распределение.
"Ящики с усами" показывают наличие выбросов во всех столбцах, кроме углов нашивки.

Матрицы диаграмм рассеяния
"""

sns.pairplot(df, height=3)
plt.tight_layout()
plt.show()

"""Из графиков видно явное наличие выбросов в большинстве признаков.

Создадим корреляционную матрицу,представляющую собой тепловую карту,в которой измеряется линейная зависимость между парой признаков.
"""

sns.set(font_scale=1)
fig, ax = plt.subplots(figsize=(12, 10))
heatMap = sns.heatmap(df.corr(),
                     cbar=True,
                     annot=True,
                     square=True,
                     fmt='.2f',
                     linewidths=1, linecolor='y',
                     annot_kws={'size': 15})
plt.xticks(rotation=45, ha='right')
plt.show()

"""Корреляция между всеми параметрами близка к 0.

Найдём выбросы.Для этого можно использовать метод 3-х сигм или метод межквартильных расстояний.
Правило трех сигм (3-sigma rule) - правило, утверждающее, что вероятность того, что случайная величина отклонится от своего математического ожидания более чем на три среднеквадратических отклонения.
Межквартильный диапазон набора данных, часто сокращенно IQR, представляет собой разницу между первым квартилем (25-й процентиль) и третьим квартилем (75-й процентиль) набора данных
"""

count_3s = 0
count_iq = 0
for column in df:
    d = df.loc[:, [column]]
    # методом 3-х сигм
    zscore = (df[column] - df[column].mean()) / df[column].std()
    d['3s'] = zscore.abs() > 3
    count_3s += d['3s'].sum()
    # методом межквартильных расстояний
    q1 = np.quantile(df[column], 0.25)
    q3 = np.quantile(df[column], 0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    d['iq'] = (df[column] <= lower) | (df[column] >= upper)
    count_iq += d['iq'].sum()
    # визуализация выбросов
    print('{}: 3s={} iq={}'.format(column, d['3s'].sum(), d['iq'].sum()))
    fig, axes = plt.subplots(1, 2, figsize=(12, 2.5))
    sns.histplot(data=d, x=column, hue='3s', multiple='stack', legend=False, ax=axes[0])
    sns.boxplot(data=d, x=column, color='tab:gray', ax=axes[1])
    sns.stripplot(data=d[d['iq']==False], x=column, ax=axes[1])
    sns.stripplot(data=d[d['iq']==True], x=column, color='tab:orange', ax=axes[1])
    plt.show()

print('Метод 3-х сигм, выбросов:', count_3s)
print('Метод межквартильных расстояний, выбросов:', count_iq)

#Создадим переменную со списком всех параметров, в которых есть выбросы
df.columns
column_list_drop = ["Соотношение матрица-наполнитель",
                 "Плотность, кг/м3",
                 "модуль упругости, ГПа",
                 "Количество отвердителя, м.%",
                 "Содержание эпоксидных групп,%_2",
                 "Температура вспышки, С_2",
                 "Поверхностная плотность, г/м2",
                 "Модуль упругости при растяжении, ГПа",
                 "Прочность при растяжении, МПа",
                 "Потребление смолы, г/м2",
                 "Шаг нашивки",
                 "Плотность нашивки"]

"""Очистим данные от выбросов методом межквартильного расстояния"""

for i in column_list_drop:
    q75, q25 = np.percentile(df.loc[:,i], [75,25])
    intr_qr = q75 - q25
    max = q75 + (1.5 * intr_qr)
    min = q25 - (1.5 * intr_qr)
    df.loc[df[i] < min, i] = np.nan
    df.loc[df[i] > max, i] = np.nan

df.shape # размерность датасета

"""Сумма выбросов по каждому столбцу"""

df.isnull().sum()

#Удаляем строки c выбросами
df = df.dropna(axis=0)

"""Сумма выбросов по каждому столбцу"""

df.isnull().sum()

df.shape # размерность датасета

df.info() # информация о датасете

"""Построим диаграмму "ящики с усами" после удаления выбросов"""

scaler = MinMaxScaler()
scaler.fit(df)
plt.figure(figsize = (20, 20))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,fontsize = 30)
plt.boxplot(pd.DataFrame(scaler.transform(df)), labels = df.columns,patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'y', color = 'y'),medianprops = dict(color = 'g'), whiskerprops = dict(color = "b"), capprops = dict(color = "black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()

"""Повторяем процедуру"""

for i in column_list_drop:
    q75, q25 = np.percentile(df.loc[:,i],[75, 25])
    intr_qr = q75 - q25
    max = q75 + (1.5*intr_qr)
    min = q25 - (1.5*intr_qr)
    df.loc[df[i] < min,i] = np.nan
    df.loc[df[i] > max,i] = np.nan
# Удаляем строки с выбросами
df = df.dropna(axis=0)

df.info() # информация о датасете

df.isnull().sum() # Пропуски

"""Доля отсутствующих записей для каждого признака"""

for col in df.columns:
    pct_missing = np.mean(df[col].isnull())
    print('{} - {}%'.format(col, round(pct_missing*100)))

"""Строим диаграмму "ящик с усам"
"""

scaler = MinMaxScaler()
scaler.fit(df)
plt.figure(figsize = (20, 20))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,fontsize = 30)
plt.boxplot(pd.DataFrame(scaler.transform(df)), labels = df.columns,patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'y', color = 'y'),medianprops = dict(color = 'g'), whiskerprops = dict(color = "b"), capprops = dict(color = "black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()

for i in column_list_drop:
    q75, q25 = np.percentile(df.loc[:,i],[75, 25])
    intr_qr = q75 - q25
    max = q75 + (1.5*intr_qr)
    min = q25 - (1.5*intr_qr)
    df.loc[df[i] < min,i] = np.nan
    df.loc[df[i] > max,i] = np.nan
# Удаляем строки с выбросами
df = df.dropna(axis=0)

"""Ящики с усами"""

scaler = MinMaxScaler()
scaler.fit(df)
plt.figure(figsize = (20, 20))
plt.suptitle('Диаграммы "ящики с усами"', y = 0.9 ,fontsize = 30)
plt.boxplot(pd.DataFrame(scaler.transform(df)), labels = df.columns,patch_artist = True, meanline = True, vert = False, boxprops = dict(facecolor = 'y', color = 'y'),medianprops = dict(color = 'g'), whiskerprops = dict(color = "b"), capprops = dict(color = "black"), flierprops = dict(color = "y", markeredgecolor = "maroon"))
plt.show()

"""Из диаграммы видно,что выбросов нет."""

df.info ()

df.isnull().sum()

a = 5
b = 5
c = 1

plt.figure(figsize=(35,35))

for col in df.columns:
    plt.subplot(a, b, c)
    #plt.figure(figsize=(7,5))
    sns.boxplot(data = df, y=df[col], fliersize=15, linewidth=5)
    plt.ylabel(None)
    plt.title(col, size = 20)
    #plt.show()
    c+=1

"""Создадим корреляционную матрицу,представляющую собой тепловую карту,в которой измеряется линейная зависимость между парой признаков."""

sns.set(font_scale=1)
fig, ax = plt.subplots(figsize=(12, 10))
heatMap = sns.heatmap(df.corr(),
                     cbar=True,
                     annot=True,
                     square=True,
                     fmt='.2f',
                     linewidths=1, linecolor='y',
                     annot_kws={'size': 15})
plt.xticks(rotation=45, ha='right')
plt.show()

# Средние и медианные знчения датасета после исключения выбросов 
mean_and_50 = df.describe()
mean_and_50.loc[['mean', '50%']]

df.shape

"""Размерность датасета изменилась"""

df

"""**Разработка и обучение модели для прогноза модуля упругости при растяжении и прочности при растяжении. **

Нормализация данных
"""

min_max_Scaler = preprocessing.MinMaxScaler()
col = df.columns
result = min_max_Scaler.fit_transform(df)
min_max_Scaler_df = pd.DataFrame(result, columns = col)
min_max_Scaler_df.describe()

plt.figure(figsize = (15,9))
ax = sns.boxplot(data = min_max_Scaler_df)
ax.set_xticklabels(ax.get_xticklabels(),rotation=30);



df_norm = pd.DataFrame(min_max_Scaler.fit_transform(df), columns = df.columns, index=df.index)
df_norm.describe().T

sns.pairplot(df_norm, hue = 'Угол нашивки', markers=["o", "s"], diag_kind= 'auto', palette='Set2')

"""Тепловая диаграмма"""

mask = np.triu(df_norm.corr())
f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(df_norm.corr(), mask=mask, annot=True, square=True, cmap='coolwarm')
plt.xticks(rotation=45, ha='right')
plt.show()

df.info()

df.isna().sum() #isna - ищет пустые значения(NaN'ы), а sum() выводит их сумму

"""Перед нами стоит задача регрессии – прогноз на основе выборки объектов с различными признаками.Построим модели для прогноза модуля упругости при растяжении и прочности при растяжении.

Разбиваем данные на обучающую и тестовую выборки.
"""

X_u = df_norm.drop(['Модуль упругости при растяжении, ГПа'], axis=1) 
X_p = df_norm.drop(['Прочность при растяжении, МПа'], axis=1)
y_u = df_norm[['Модуль упругости при растяжении, ГПа']]
y_p = df_norm[['Прочность при растяжении, МПа']]

X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(X_u, y_u, test_size=0.3, random_state=42)
X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_p, y_p, test_size=0.3, random_state=42)

# Размерность 
print(X_train_u.shape)
print(X_test_u.shape)

print(y_train_u.shape)
print(y_test_u.shape)

print(X_train_p.shape)
print(X_test_p.shape)

print(y_train_p.shape)
print(y_test_p.shape)

"""Линейная регрессия"""

model = LinearRegression()
model.fit(X_train_u,y_train_u)
predictions = model.predict(X_test_u)
r_sq = model.score(X_u, y_u)
print('coefficient of determination:', r_sq) # коэффициент детерминации. R2 (или Коэффициент детерминации) — это статистическая мера, которая показывает степень вариации зависимой переменной из-за независимой переменной.
print(
  'mean_squared_error : ', mean_squared_error(y_test_u, predictions)) #Средняя квадратическая ошибка
print(
  'mean_absolute_error : ', mean_absolute_error(y_test_u, predictions)) #Средняя абсолютная ошибка

print(model.coef_) # массив весов, оцененных с помощью линейной регрессии.

print('intercept:', model.intercept_) # Скалярный отрезок прямой
print('slope:', model.coef_) # Массив

"""Модель предсказания"""

y_pred_u = model.predict(X_u)
print('predicted response:', y_pred_u, sep='\n')

plt.figure(figsize = (10, 7))
plt.title("Линейная регрессия")
plt.plot(predictions, label = "Прогноз", color = "orange")
plt.plot(y_test_u.values, label = "Тест", color = "b")
plt.xlabel("Количество наблюдений")
plt.ylabel("Модуль упругости, ГПа")
plt.grid(True);
plt.show()

model = LinearRegression()
model.fit(X_train_p,y_train_p)
predictions = model.predict(X_test_p)
r_sq = model.score(X_p, y_p)
print('coefficient of determination:', r_sq) # коэффициент детерминации
print(
  'mean_squared_error : ', mean_squared_error(y_test_p, predictions)) #средняя квадратическая ошибка
print(
  'mean_absolute_error : ', mean_absolute_error(y_test_p, predictions)) #Средняя абсолютная ошибка

print(model.coef_)

print('intercept:', model.intercept_) # Скалярный отрезок прямой
print('slope:', model.coef_) # Массив

y_pred_p = model.predict(X_p)
print('predicted response:', y_pred_p, sep='\n')

plt.figure(figsize = (10, 7))
plt.title("Линейная регрессия")
plt.plot(predictions, label = "Прогноз", color = "orange")
plt.plot(y_test_p.values, label = "Тест", color = "b")
plt.xlabel("Количество наблюдений")
plt.ylabel("Прочность при растяжении, МПа")
plt.grid(True);
plt.show()

"""Метод k-ближайших соседей"""

knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_u, y_train_u)
y_pred_u_knn = knn.predict(X_test_u)
mae_knr = mean_absolute_error(y_pred_u_knn, y_test_u)
mse_knn_elast = mean_squared_error(y_test_u, y_pred_u_knn)

print('K Neighbors Regressor  Results Train:') 
print("Test score: {:.2f}".format(knn.score(X_train_u, y_train_u)))# Оценка тренировочной выборки
print('K Neighbors Regressor  Results:') # результаты регрессора соседей
print('KNN_MAE: ', round(mean_absolute_error(y_test_u, y_pred_u_knn))) # Средняя абсолютная ошибка
print('KNN_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_u, y_pred_u_knn))) # Средняя абсолютная ошибка в процентах
print('KNN_MSE: {:.2f}'.format(mse_knn_elast)) # cредняя квадратическая ошибка
print("KNN_RMSE: {:.2f}".format (np.sqrt(mse_knn_elast))) # Среднеквадратическая ошибка
print("Test score: {:.2f}".format(knn.score(X_test_u, y_test_u)))# Оценка тестовой выборки

"""GridSearch — поиск лучших параметров в фиксированной сетке возможных значений.

CV – перекрёстная проверка (кросс-валидация, Cross-validation), метод, который показывает, что модель не переобучилась.
"""

knr = KNeighborsRegressor()
knr_params = {'n_neighbors' : range(1, 301, 2), 
          'weights' : ['uniform', 'distance'],
          'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']
          }
GSCV_knr_u = GridSearchCV(knr, knr_params, n_jobs=-1, cv=10)
GSCV_knr_u.fit(X_train_u, y_train_u)
GSCV_knr_u.best_params_

knr_u = GSCV_knr_u.best_estimator_
print(f'R2-score Модуль упругости при растяжении: {knr_u.score(X_test_u, y_test_u).round(3)}')

"""R2<0-разработанная модель даёт прогноз даже хуже, чем простое усреднение."""

knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_p, y_train_p)
y_pred_p_knn = knn.predict(X_test_p)
mae_knr = mean_absolute_error(y_pred_p_knn, y_test_p)
mse_knn_elast = mean_squared_error(y_test_p, y_pred_p_knn)

print('K Neighbors Regressor  Results Train:') 
print("Test score: {:.2f}".format(knn.score(X_train_p, y_train_p)))# Оценка тренировочной выборки
print('K Neighbors Regressor  Results:') # результаты регрессора соседей
print('KNN_MAE: ', round(mean_absolute_error(y_test_p, y_pred_p_knn))) # Средняя абсолютная ошибка
print('KNN_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_p, y_pred_p_knn))) # Средняя абсолютная ошибка в процентах
print('KNN_MSE: {:.2f}'.format(mse_knn_elast)) # cредняя квадратическая ошибка
print("KNN_RMSE: {:.2f}".format (np.sqrt(mse_knn_elast))) # Среднеквадратическая ошибка
print("Test score: {:.2f}".format(knn.score(X_test_p, y_test_p)))# Оценка тестовой выборки

knr = KNeighborsRegressor()
knr_params = {'n_neighbors' : range(1, 301, 2), 
          'weights' : ['uniform', 'distance'],
          'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']
          }
GSCV_knr_p = GridSearchCV(knr, knr_params, n_jobs=-1, cv=10)
GSCV_knr_p.fit(X_train_p, y_train_p)
GSCV_knr_p.best_params_

knr_p = GSCV_knr_p.best_estimator_
print(f'R2-score Прочность при растяжении: {knr_p.score(X_test_p, y_test_p).round(3)}')

"""Случайный лес

RandomizedSearchCV реализует метод «подгонки» и «оценки».В отличие от GridSearchCV, проверяются не все значения параметров, а из указанных распределений выбирается фиксированное количество значений параметров.
"""

from sklearn.model_selection import RandomizedSearchCV

rfr = RandomForestRegressor(n_estimators=15, max_depth=7, random_state=33)
rfr.fit(X_train_u, y_train_u.values)
y_pred_u_forest = rfr.predict(X_test_u)
mae_rfr = mean_absolute_error(y_pred_u_forest, y_test_u)
mse_rfr_elast = mean_squared_error(y_test_u, y_pred_u_forest)

print('Random Forest Regressor Results Train:')
print("Test score: {:.2f}".format(rfr.score(X_train_u, y_train_u))) 
print('Random Forest Regressor Results:')
print('RF_MAE: ', round(mean_absolute_error(y_test_u, y_pred_u_forest)))
print('RF_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_u, y_pred_u_forest)))
print('RF_MSE: {:.2f}'.format(mse_rfr_elast))
print("RF_RMSE: {:.2f}".format (np.sqrt(mse_rfr_elast)))
print("Test score: {:.2f}".format(rfr.score(X_test_u, y_test_u)))

rfr = RandomForestRegressor()
rfr_params = {
    'n_estimators' : range(10, 1000, 10),
    'criterion' : ['squared_error', 'absolute_error', 'poisson'],
    'max_depth' : range(1, 7),
    'min_samples_split' : range(20, 50, 5),
    'min_samples_leaf' : range(2, 8),
    }
RSCV_rfr_u = RandomizedSearchCV (rfr, rfr_params, n_jobs=-1, cv=10,verbose=4)
RSCV_rfr_u.fit(X_train_u, np.ravel(y_train_u))
RSCV_rfr_u.best_params_

rfr_u = RSCV_rfr_u.best_estimator_
print(f'R2-score Модуль упругости при растяжении: {rfr_u.score(X_test_u, y_test_u).round(3)}')

rfr = RandomForestRegressor(n_estimators=15, max_depth=7, random_state=33)
rfr.fit(X_train_p, y_train_p.values)
y_pred_p_forest = rfr.predict(X_test_p)
mae_rfr = mean_absolute_error(y_pred_p_forest, y_test_p)
mse_rfr_elast = mean_squared_error(y_test_p, y_pred_p_forest)

print('Random Forest Regressor Results Train:')
print("Test score: {:.2f}".format(rfr.score(X_train_p, y_train_p))) 
print('Random Forest Regressor Results:')
print('RF_MAE: ', round(mean_absolute_error(y_test_p, y_pred_p_forest)))
print('RF_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_p, y_pred_p_forest)))
print('RF_MSE: {:.2f}'.format(mse_rfr_elast))
print("RF_RMSE: {:.2f}".format (np.sqrt(mse_rfr_elast)))
print("Test score: {:.2f}".format(rfr.score(X_test_p, y_test_p)))

rfr = RandomForestRegressor()
rfr_params = {
    'n_estimators' : range(10, 1000, 10),
    'criterion' : ['squared_error', 'absolute_error', 'poisson'],
    'max_depth' : range(1, 7),
    'min_samples_split' : range(20, 50, 5),
    'min_samples_leaf' : range(2, 8),
    }
RSCV_rfr_p = RandomizedSearchCV (rfr, rfr_params, n_jobs=-1, cv=10,verbose=4)
RSCV_rfr_p.fit(X_train_p, np.ravel(y_train_p))
RSCV_rfr_p.best_params_

rfr_p = RSCV_rfr_p.best_estimator_
print(f'R2-score Прочность при растяжении: {rfr_p.score(X_test_p, y_test_p).round(3)}')

"""Написание нейронной сети, которая будет рекомендовать соотношение матрица-наполнитель. """

# Определяем входы и выходы
X_MN = df.drop(['Соотношение матрица-наполнитель'], axis=1)
y_MN = df[['Соотношение матрица-наполнитель']]
# Разбиваем выборки на обучающую и тестовую
X_train_MN, X_test_MN, y_train_MN, y_test_MN = train_test_split(X_MN, y_MN, test_size=0.3, random_state=1)

df

"""Нормализация"""

x_array = np.array(df['Соотношение матрица-наполнитель']) 
normalized_arr = preprocessing.normalize([x_array])
print(normalized_arr)

"""Слои нейронной сети"""

modelMN = Sequential()
modelMN.add(Dense(128))
modelMN.add(BatchNormalization())
modelMN.add(ReLU())
modelMN.add(Dense(128, activation='selu')) #SELU-это функция активации сочетает в себе оба преимущества классического RELU со свойствами самонормализации.
modelMN.add(BatchNormalization())
modelMN.add(Dense(64, activation='selu'))
modelMN.add(BatchNormalization())
modelMN.add(Dense(32, activation='selu'))
modelMN.add(BatchNormalization())
modelMN.add(ReLU())
modelMN.add(Dense(16, activation='selu'))
modelMN.add(BatchNormalization())
modelMN.add(Dense(1))
modelMN.add(Activation('selu'))

"""Построение модели и определение её параметров

SGD-стохастический градиентный спуск с мини-пакетами — вариант, при котором коэффициенты меняются после обсчета N элементов выборки, то есть для каждой тренировочной итерации алгоритм выбирает случайное подмножество набора данных.Частота обновления параметров выше,меньше требуется оперативной памяти,эффективность вычислений высокая.
"""

modelMN.compile(optimizer=tf.optimizers.SGD(learning_rate=0.02, momentum=0.5),
    loss='mean_absolute_error',metrics=['mse', "mape"])

tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=10,
    verbose=1,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0
)# Параметры ранней остановки для претотвращения "переоснащения" модели.

# Минимизируемая функция потерь
loss=keras.losses.SparseCategoricalCrossentropy()

historyMN=modelMN.fit(
    X_train_MN,
    y_train_MN,
    batch_size = 64,
    epochs=100,
    verbose=1,
    validation_split = 0.2,
        )

"""Оценим модель"""

modelMN.evaluate(X_test_MN, y_test_MN)

scores = modelMN.evaluate(X_train_MN,y_train_MN)
print("\n%s: %.2f%%" % (modelMN.metrics_names[1], scores[1]*100))

historyMN.history

modelMN.summary()

"""Прогноз"""

y_pred_model = modelMN.predict(X_test_MN)

print('Model Results:')
print('Model_MAE: ', round(mean_absolute_error(y_test_MN, y_pred_model)))
print('Model_MAPE: {:.2f}'.format(mean_absolute_percentage_error(y_test_MN, y_pred_model)))
print("Test score: {:.2f}".format(mean_squared_error(y_test_MN, y_pred_model)))

def model_loss_plot(model_hist):
    plt.figure(figsize = (17,5))
    plt.plot(historyMN.history['loss'],
             label = 'ошибка на обучающей выборке')
    plt.plot(historyMN.history['val_loss'],
            label = 'ошибка на тестовой выборке')
    plt.title('График потерь модели')
    plt.ylabel('Значение ошибки')
    plt.xlabel('Эпохи')
    plt.legend(['Oшибка на обучающей выборке', 'Ошибка на тестовой выборке'], loc='best')
    plt.show()
model_loss_plot(historyMN)

def actual_and_predicted_plot(orig, predict, var, model_name):    
    plt.figure(figsize=(17,5))
    plt.title(f'Тестовые и прогнозные значения: {model_name}')
    plt.plot(orig, label = 'Тест')
    plt.plot(predict, label = 'Прогноз')
    plt.legend(loc = 'best')
    plt.ylabel(var)
    plt.xlabel('Количество наблюдений')
    plt.show()
actual_and_predicted_plot(y_test_MN.values, model.predict(X_test_MN.values), 'Cоотношение матрица/наполнитель', 'Keras_neuronet')

"""Средняя абсолютная ошибка"""

print(f'Model MAE: {modelMN.evaluate(X_test_MN, y_test_MN)}')

print(f'MAE среднего значения: {np.mean(np.abs(y_test_MN-np.mean(y_test_MN)))}')

"""Сохранение модели"""

with open ('historyMN.pkl','wb') as f:  
            pickle.dump(historyMN,f)